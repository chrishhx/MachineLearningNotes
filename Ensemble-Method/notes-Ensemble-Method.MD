# 集成方法/集成学习 Ensemble Method/Ensemble learning

## 概述
集成学习是一种使用多个模型处理同一个问题的模板/范例。一般机器学习算法从训练数据中学习到一个特定假设模型（Hypothesis），而集成学习则试图从训练数据中学习到一组特定假设模型并结合它们。

## 基本概念
弱模型（weak/base learner）：稍好于瞎猜(50%)的模型，准确率在五六成的模型，具体视任务而定;
强模型（strong learner）：能提供准确预测的模型，准确率一般在八成以上，具体视任务而定;
集成方法（ensemble method）：给定多个基础模型 $h_1,h_2,...,h_m$ ，组合为一个模型的函数$f$ ，$H = f(h_1,...,h_m)$
集成学习（ensembles）：构建多个弱模型，用某种策略将多个结果集成起来，作为最终结果。

实际上大多数模型都可以通过特殊约束构造弱分类器，从而作为集成学习的基模型，常见的弱模型有：分类树/回归树，神经网络，线性分类器；

集成方法非常丰富，既有串行的也有并行的，从最基本的多数投票到建立一个新的的模型来输出结果都可以使用，常见的方法主要是：Bagging，Boosting，Stacking；

集成学习的性能主要与以下两点有关：
1. 基模型的性能与**多样性**；
2. 集成方法的选取，包括集成方法的参数调节，例如基模型的个数。

## Bagging

### 自助法 Bootstrap
在统计学中，自助法（BootstrapMethod，Bootstrapping或自助抽样法）可以指任何一种有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。

### Bootstrap Aggregating
Bagging 的全称是 Bootstrap Aggregating ，顾名思义就是利用自助法抽取多组样本，然后分组训练多个模型然后将结果聚合，由于多组样本训练得到的模型是独立的，所以Bagging实际上可以并行地训练多个基模型。

Bagging的算法流程：
1. 用自助法产生 $b$ 组样本 $S_1,...,S_b$
2. 对应每一组样本训练 $b$ 个基模型 $h_1,...,h_b$
3. 简单投票法（分类）或者简单平均法（回归）组合基模型得到 $H = f(h_1,...,h_b)$

未被采样的样本，称为包外样本。可用作验证集。若为决策树，可辅助剪枝，辅助对0结点的处理。若为神经网络，可辅助early stopping，减小过拟合。

Bagging 关注于降低方差，因此在不剪枝决策树、神经网络等易受样本扰动的学习器上效果明显。随机森林一般归类到Bagging中，相比一般的Bagging，随机森体添加了特征采样（列采样）来增加基模型的多样性。

## Boosting
实际上，Boosting是一系列加法模型，模型的通用表示为 $H = \alpha_1 h_1 + ... + \alpha_b h_b$ ，Boosting有许多变体，诸如AdaBoost，GBM，XGBoost等,与Bagging不同，Boosting按顺序训练多个基模型，训练第 $i$ 个模型时，不仅依赖训练集，也依赖前面训练得到的 $i-1$ 个模型 $h_j$ 和权值参数 $\alpha_j$.

Boosting的算法流程：
1. 初始化
2. 基于前面的训练结果与训练集，训练模型 $h_i$，通过特定规则算出 $\alpha_i$
3. 将模型 $h_i$ 组装到总体模型中 $H_i = H_{i-1} + \alpha_i h_i$
4. 重复 (2),(3) $b$ 次，得到最终模型 $H = H_b = \alpha_1 h_1 + ... + \alpha_b h_b $

AdaBoost是最早实用的一个boosting算法，后续发现可以归类在Gradient Boosting Machine中 [Friedman, J. H. (2001)](GBM.pdf)，而XGBoost则可以看作GBM的一个改进，引入了二阶近似和列采样以及一系列优化方法 [Chen, T., & Guestrin, C. (2016, August)](XGBoost.pdf)

## Stacking
Bagging 和 Boosting 在集成模型上选择的都是简单的求和/加权和，而 Stacking 则在集成算法上做文章，通过一个特定的模型 meta-learner 将基模型集成到一起。

Stacking的算法流程：
0. 训练集 $D_0 = \{ (x_i,y_i) | i=1,...n \}$
1. 训练 $t$ 个基模型 $h_1,...,h_t$ ，训练模型时也可以做Boostrap/特征采样等操作来提升多样性
2. 对训练集的第 $i$ 个样本 $(x_i)$ 和 第 $j$ 个基模型，获得predict的结果 $z_{i,j} = h_j(x_i)$ , $z_i = (z_{i,1},...z_{i,T})^T$
3. 将 $ D=\{ (z_i,y_i) | i=1,...n \} $ 作为新的训练集，训练 meta-learner $h$
4. 得到最终模型 $ H = h ( h_1(x),h_2(x),...,h_t(x) ) $

Bagging 和 Boosting 在多数情况下使用的都是同构的基模型，这是由于两者在基模型的假设上需要让每个基模型拥有差不多的地位，而Stacking则没有这样的问题，大可使用多组不同构的基模型，假如训练出来的某些基模型特别没用，那么在训练meta-learner时完全可以用变量选择的方法将多余的基模型去掉。

如果基模型选择使用 Stacking 模型，则可以多层堆叠，甚至能得到类似深度网络的结构，但适用性还有待进一步研究，

## 为什么集成要比单个好
[Dietterich, T. G. (1997)](MLresearch.pdf) 中给出了几个可能的解释。将机器学习看作是在一个假设空间（hypothesis space）中寻找最准确的假设的过程，那么，集成算法的优势则可以看作：
1. 训练集可能没有提供选出最佳单一模型所需的充分的信息，例如一个假设空间中有多个表现相近的模型 $h_1,...,h_t$，那么，比起只用其中一个模型，集成是更好的选择
2. 训练模型的过程可能无法找到最优解，这是显而易见的，由于机器学习中存在着大量的启发式的贪心算法，无法保证找到的解一定是最优的，而多个非最优解的妥协可能是一个更好的结果
3. 假设空间可能不包含真实模型，即存在Bias，集成学习可以令模型尽量靠近假设空间内最佳的那个模型。